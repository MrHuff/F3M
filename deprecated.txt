template<typename T>
//void host_to_cuda(T *cuda_pointer, T* const host_pointer ,int N,int M){
//    cudaMemcpy(cuda_pointer, host_pointer, sizeof(T)*N*M, cudaMemcpyHostToDevice);
//};
//template<typename T>
//void cuda_to_host(T* const cuda_pointer, T *host_pointer ,int N,int M){
//    cudaMemcpy(host_pointer,cuda_pointer, sizeof(T)*N*M, cudaMemcpyDeviceToHost);
//};
//template<typename T>
//T* cuda_to_host_and_pointer(T* const cuda_pointer ,int N,int M){
//    T *host_pointer = new T[N*M];
//    cudaMemcpy(host_pointer,cuda_pointer, sizeof(T)*N*M, cudaMemcpyDeviceToHost);
//    return host_pointer;
//};
//
//template<typename T>
//T* allocate_cuda_mat(int N,int M){
//    T *d;
//    cudaMalloc((void **) &d, sizeof(T)*N*M);
//    return d;
//};
//
//void print_row_mat(float ** mat, int N, int M){
//    for (int i = 0; i < N; i++) {
//        printf("row %i :",i);
////        float * tmp = mat[i];
//        for (int j = 0; j < M; j++) {
//            printf(" %f ",mat[i][j]);
//            }
//        printf("\n");
//        }
//}
//
//void print_mat(std::ostream& out,float* const mat, int N, int M){
//    for (int i = 0; i < N; i++) {
//        printf("row %i :",i);
//        for (int j = 0; j < M; j++) {
//            printf(" %f ",mat[i * M + j]);
//            if (j==(M-1)){
//                printf("\n");
//            }
//
////            printf("i: %i, j: %i, val: %f \n",i,j,mat[i * M + j]);
////            out << " " << mat[i * M + j]; //Most cancer thing about C++ and cuda. Row major and column major matrices. Imagine tensors rip.
////            out << " " << i * M + j;
////            printf("i: %i, j: %i ",i,j);
//        }
//    }
//};
//
////Stare at keops code, think idea is to do 1d block calculations and let each block "iterate over the data" in y direction
////
//template <typename T>
//__global__ void print_mat_cuda(T* const mat){
//    int i = threadIdx.x+blockIdx.x*blockDim.x;
//    if (i>nx*nd-1){return;}
//    printf("%i: %f \n",i,mat[i]);
//}
//float** generate_row_random_matrix(int const N, int const M){
//    std::random_device rd;
//    std::mt19937 gen(rd());
//    std::normal_distribution<float> dis(0, 1);
//    auto ** A = new float*[N];
//    A[0] = new float[N * M];
//    for (int i = 1; i < N; ++i) A[i] = A[i-1] + M;
//
//    for (int i = 0; i < N; ++i) {
//        for (int j = 0; j < M; ++j) {
////            A[i][j] = i*M+j;
//            A[i][j] = dis(gen);
//
//        }
//    }
//    return A;
//}
//
//
//float* generate_row_major_random_matrix(int const N, int const M){
//    std::random_device rd;
//    std::mt19937 gen(rd());
//    std::normal_distribution<float> dis(0, 2);
//    auto *mat = new float[N*M];
//    for (int i = 0; i < N; i++) { //for rows
//        for (int j = 0; j < M; j++) { //for columns, M = width
//            mat[i * M + j] = dis(gen);
////            printf("i: %i, j: %i, val: %f \n",i,j,mat[i * M + j]);
//        }
//    }
//    return mat;
//}

//REMEMBER TO ALWAYS CHECK EVERYTHING IS ACTUALLY ON CUDA!
//blockDim,gridDim = give dim of block, grid
//blockIdx,threadIdx = is specific index of thread and block. Grid have no idx obv
//The execution configuration (of a global function call) is specified by inserting an expression of the form <<<Dg,Db,Ns,S>>>, where:
//
//Dg (dim3) specifies the dimension and size of the grid.
//Db (dim3) specifies the dimension and size of each block
//Ns (size_t) specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory.
//S (cudaStream_t) specifies the associated stream, is an optional parameter which defaults to 0.
// figure out way to provide constant dimensions
// Compute on device : grid and block are both 1d
//GpuConv1D_ranges.cu block sparse version
//GpuConv1D.cu regular
//Maybe start with naive global memory loop
//Do simulated 2-d, flatten is the way to go.

/*
//__device__ static void load(int c, float *xi, const float *px) {
//    assert(xi != nullptr);
//    assert(px != nullptr);
//    /*
//     * px is an "array" of pointers to data arrays of appropriate sizes.
//     * That is, px[0] = *px     is a pointer to a TYPE array of size Ni * FIRST
//     * Then,    px[1] = *(px+1) is a pointer to a TYPE array of size Ni * NEXT::FIRST; etc.
//     *
//     * (where Ni is the max value of "i" you should expect)
//     * Obviously, we do not make any sanity check... so beware of illicit memory accesses !
//     */
//#pragma unroll
//    for (int k = 0; k < nd; k++) {
//        //assert(&((*px)[i * FIRST + k]) != nullptr);
//        xi[nd*threadIdx.x+k] = px[c*nd + k]; // First, load the i-th line of px[0]  -> xi[ 0 : FIRST ].
//        // Don't use thread id -> nvidia-chips doesn't work like that! It only got a third of the way
//        // Some weird memory allocation issue
//    }
//}
//
//
//__global__ void rbf_1d_kernel_shared(const float *input_x,const float *input_y, float *output) {
//    int i = blockIdx.x * blockDim.x + threadIdx.x; // current thread
//    float x_i[nd];
//    extern __shared__ float yj[];
//    for (int k = 0; k < nd; k++) {
//        x_i[k] = input_x[i * nd + k];
//    }
//    for (int jstart = 0, tile = 0; jstart < ny; jstart += blockDim.x, tile++) {
//        // get the current column
//        int j = tile * blockDim.x + threadIdx.x; //periodic threadIdx.x you dumbass.
//
//
//
//        if (j < ny) { // we load yj from device global memory only if j<ny
//            load(j,yj,input_y);
//        }
//        __syncthreads();
//
//
////                printf("%f %f %f \n",yj[0],yj[1],yj[2]);
////        printf("%f %f %f \n",yj[3],yj[4],yj[5]);
//        if (i < nx) { // we compute x1i only if needed
//            float *yjrel = yj; // Loop on the columns of the current block.
//            for (int jrel = 0; (jrel < blockDim.x) && (jrel < ny - jstart); jrel++, yjrel += nd) {
//                output[i*ny+jrel+tile*blockDim.x] = rbf_simple(x_i,yjrel);
////                printf(" %f \n",yjrel[0]);
//            }
//            __syncthreads();
//        }
//    };
//}
//
//
//__global__ void rbf_1d_reduce_shared(const float *input_x, const float *input_y, const float *b, float *output) {
//    int i = blockIdx.x * blockDim.x + threadIdx.x; // current thread
//    float x_i[nd];
//    float acc = 0.0;
//    extern __shared__ float yj[];
////    printf("thread% i \n",i);
//    if (i<nx) {
//        for (int k = 0; k < nd; k++) {
//            x_i[k] = input_x[i * nd + k];
//        }
//    }
//    for (int jstart = 0, tile = 0; jstart < ny; jstart += blockDim.x, tile++) {
//        // get the current column
////        printf("%i \n",jstart); //Sums incorrectly!
//        int j = tile * blockDim.x + threadIdx.x; //periodic threadIdx.x you dumbass. 0-3 + 0-2*4
////        printf("%i \n",j); //synchronization error! some threads in block go ahead and jump across tiles!
////        if (blockIdx.x==5){
////            printf("%i \n", tile);
////
////        }
//        if (j < ny) { // we load yj from device global memory only if j<ny
////            printf("thread% i \n",i);
//            load(j,yj,input_y);
//        }
//        __syncthreads();
////        if (i==960){ //Some sort of memory misallocation for block 5...
//////            printf("%i \n",j); //a subset of y_data is not loaded for two threads?
////            float *yjrel = yj; // Loop on the columns of the current block.
////        for (int jrel = 0; (jrel < blockDim.x) && (jrel < ny - jstart); jrel++, yjrel += nd) {
////            printf("%i: y: %f  %f  %f  \n", jrel + jstart, yjrel[0], yjrel[1],
////                   yjrel[2]); //pointer yjrel acting funny, not pointing to right place in data! Remember 64 + 192 = 256!
////            printf("%i: y: %f  %f  %f  \n", jrel + jstart, yj[192], yj[193],
////                   yj[194]); //pointer yjrel acting funny, not pointing to right place in data! Remember 64 + 192 = 256!
////        }
////        }
//
//        //ok maybe its not top prio to fix this, maybe just use even threads for good reference...
//        if (i < nx) { // we compute x1i only if needed
//            float *yjrel = yj; // Loop on the columns of the current block.
//            for (int jrel = 0; (jrel < blockDim.x) && (jrel < ny - jstart); jrel++, yjrel += nd) {
//                acc+= rbf_simple(x_i,yjrel)*b[jrel+jstart]; //sums incorrectly cause pointer is fucked not sure if allocating properly
////            if (i==960){ // some thread specific error
////
//////                printf("%f %f %f  \n",yjrel[0],yjrel[1],yjrel[2]); //For some reason gives incorrect
////////                printf("%f %f %f  \n",x_i[0],x_i[1],x_i[2]);
//////                printf("k=%f  b=%f  \n",rbf_simple(x_i,yjrel),b[jrel+jstart]);
////                printf("%i: %f y: %f  %f  %f  \n",jrel+jstart,acc,yjrel[0],yjrel[1],yjrel[2]); //pointer yjrel acting funny, not pointing to right place in data! Remember 64 + 192 = 256!
////
////            }
//
//            }
//        }
//        __syncthreads(); //Lesson learned! Thread synching really important for cuda programming and memory loading when indices are dependent on threadIdx.x!
//
//    };
//    if (i < nx) {
//        output[i] = acc;
//    }
//}
//
//__global__ void rbf_1d_reduce_simple(const float *input_x, const float *input_y, const float *b, float *output){
//    int i = blockIdx.x * blockDim.x + threadIdx.x; // current thread
//    if (i>nx-1){return;}
//    float x_i[nd];
//    float y_j[nd];
//    float acc=0.0;
////    printf("thread% i \n",i);
//    for (int k=0;k<nd;k++){
//        x_i[k] = input_x[i*nd+k];
//    }
//    for (int p=0;p<ny;p++){
//        for (int k=0;k<nd;k++){
//            y_j[k] = input_y[p*nd+k];
//        };
//        acc+= rbf_simple(x_i,y_j)*b[p];
//    };
//    output[i] = acc;
//};
//__global__ void rbf_1d_kernel(const float *input_x,const float *input_y, float *output){
//    int i = blockIdx.x * blockDim.x + threadIdx.x; // current thread
//    if (i>nx-1){return;}
//    float x_i[nd];
//    float y_j[nd];
////    printf("thread% i \n",i);
//
//    for (int k=0;k<nd;k++){
//        x_i[k] = input_x[i*nd+k];
//    }
//
//    for (int p=0;p<ny;p++){
//        for (int k=0;k<nd;k++){
//            y_j[k] = input_y[p*nd+k];
//        };
//        output[i*ny+p] = rbf_simple(x_i,y_j);
//    };
//};
////
//
//#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
//inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {
//    if (code != cudaSuccess) {
//        fprintf(stderr, "Error: %s %s %d\n", cudaGetErrorString(code), file, line);
//        if (abort) throw new std::runtime_error("");
//    }
//}
//template<typename T>
//struct cumatrix {
//
//    T *elemns;
//    T *d_elemns;
//    bool in_device = false;
//    int N, M;
//
//    cumatrix(int n, int m) {
//        N = n; M = m;
//        elemns = new T[N * M];
//        fill_rand();
//        std::cout<<"constructor"<<std::endl;
//
//    }
//    cumatrix(const cumatrix<T>& copy_matrix) //copy constructor
//    : N(copy_matrix.N),M(copy_matrix.M),in_device(copy_matrix.in_device)
//    {
//        std::cout<<"copy"<<std::endl;
//        std::cout<<N<<std::endl;
//        std::cout<<M<<std::endl;
//        std::cout<<in_device<<std::endl;
//        elemns = new T[N * M];
//        gpuErrchk(cudaMemcpy(elemns,copy_matrix.elemns,N*M* sizeof(T),cudaMemcpyHostToHost)) //Use cuda functions for copying stuff!!
//        if (in_device){
//            std::cout<<"cuda copy"<<std::endl;
//            gpuErrchk(cudaMalloc((void **) &d_elemns, N * M * sizeof(T)));
//            gpuErrchk(cudaMemcpy(d_elemns,copy_matrix.d_elemns,N*M* sizeof(T),cudaMemcpyDeviceToDevice))
//        }
//    }
//
//    cumatrix(cumatrix<T> &&obj) noexcept {
//        std::cout<<"move constructor"<<std::endl;
//        this->elemns = obj.elemns;
//        this->d_elemns = obj.d_elemns;
//        in_device = obj.in_device;
//        N=obj.N;
//        M=obj.M;
//        obj.elemns = nullptr;
//        obj.d_elemns = nullptr;
//    }
//
//    cumatrix& operator=(const cumatrix<T> &rhs){ //actually copies the bad boy
//        std::cout<<"copy ass"<<std::endl;
//        return *this = cumatrix<T>(rhs);
//    }
//
//    cumatrix& operator=(cumatrix<T>&& other) noexcept{ //rvalues only!
//        std::cout<<"move ass"<<std::endl;
//        if (this != &other)
//        {
//            // Free the existing resource.
//            delete[] elemns; //well you have to remove what you have to replace it
//            this->release_device_data(); //well you have to remove what you have to replace it
//
//            // Copy the data pointer and its length from the
//            // source object.
//            elemns = other.elemns;
//            if (other.in_device){
//                d_elemns = other.get_device_pointer();
//                in_device = other.in_device;
////                other.release_device_data(); //dont destroy other's data! we just wanna move it around!
//            }
//            N = other.N;
//            M = other.M;
//            // Release the data pointer from the source object so that
//            // the destructor does not free the memory multiple times.
//            other.elemns = nullptr;
//            other.d_elemns = nullptr;
//        }
//        return *this;
//    }
//
//    ~cumatrix() {
//        std::cout<<"destroy"<<std::endl;
//        if (d_elemns != nullptr){
//            release_device_data();
//        }
//        if (elemns != nullptr){
//            delete [] elemns;
//        }
//    }
//    constexpr int size() const noexcept { return (N * M); }
//    constexpr int rows() const noexcept { return N; }
//    constexpr int cols() const noexcept { return M; }
//    T& operator[](int n) { return elemns[n]; }
//    const T& operator[](int n) const { return elemns[n]; }
//    T& operator()(int x, int y) { return elemns[x * M + y]; }
//    const T& operator()(int x, int y) const {  return elemns[x * M + y]; }
//    T *data() noexcept { return elemns; }
//    T* get_device_pointer(bool copy = true)
//    {
//        if (!in_device) {
//            gpuErrchk(cudaMalloc((void **) &d_elemns, N * M * sizeof(T)));
//            if (copy) gpuErrchk(cudaMemcpy(d_elemns, elemns, N * M * sizeof(T), cudaMemcpyHostToDevice));
//            in_device = true;
//        }
//        return d_elemns;
//    };
//    void refresh_from_device()
//    {
//        if (in_device) gpuErrchk(cudaMemcpy(elemns, d_elemns, N * M * sizeof(T), cudaMemcpyDeviceToHost));
//    }
//    ;
//    void refresh_to_device()
//    {
//        if (in_device) gpuErrchk(cudaMemcpy(d_elemns, elemns, N * M * sizeof(T), cudaMemcpyHostToDevice));
//    };
//    void release_device_data()
//    {
//        if (in_device) {
//            gpuErrchk(cudaFree(d_elemns));
//            in_device = false;
//        }
//    };
//    void fill_rand(){
//        std::random_device rd;
//        std::mt19937 gen(rd());
//        std::normal_distribution<T> dis(0, 1);
//        for (int i = 0; i < size(); i++)
//            (*this)[i] = dis(gen);
//    };
//};
//template<typename T>
//cumatrix<T> operator*(cumatrix<T>& a, cumatrix<T>& b) {
//    cumatrix<T> output(a.rows(), b.cols());
//    const T *d_a = a.get_device_pointer();
//    const T *d_b = b.get_device_pointer();
//    T *d_c = output.get_device_pointer(false);
//    int N = a.rows(), M = b.cols(), K = a.cols();
//    int lda = N, ldb = K, ldc = N;
//    const float alpha = 1;
//    const float beta = 0;
//
//    cublasHandle_t handle;
//    cublasCreate(&handle);
//
//    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, M, N, K, &alpha, d_a, lda, d_b, ldb, &beta, d_c, ldc);
//
//    cublasDestroy(handle);
//    output.refresh_from_device();
//
//
//    return output;
//}
//template<typename T>
//std::ostream& operator << (std::ostream& out, const cumatrix<T>& mat){
//    for (int i = 0; i < mat.rows(); i++) {
//        for (int j = 0; j < mat.cols(); j++) {
//            out << " " << mat(i, j);
//        }
//        out << std::endl;
//    }
//    return out;
//}
//template<typename T>
//cumatrix<T> test_move_constructor(cumatrix<T> a)
//{
//    return a;
//}
//    torch::Tensor rip;



//    std::cout<< X.index({ids})<<std::endl; //get row slices...

//    std::cout<<edge<<std::endl;
//    std::cout<<xmin<<std::endl;
//    std::cout<<ymin<<std::endl;

    //    print_torch_cuda_1D<float><<<gridSize,blockSize,blockSize.x * (nd) * sizeof(float)>>>(tensor_cuda_a);
    /*
    int output_dim = 2;



    /** 1-d conv chunk of code

    print_row_mat(mat,nx,nd); //figure out how to move pointer of pointer to device!
    print_mat(std::cout,mat,N,M);
    auto cuda_mat_res = allocate_cuda_mat<float>(nx,ny);

    auto mat = generate_row_random_matrix(nx, nd); //matrix but contigous in the first element!
    print_row_mat(mat,nx,nd);
    auto cuda_mat_x = allocate_cuda_mat<float>(nx,nd);
    host_to_cuda(cuda_mat_x,mat[0],nx,nd); //moves row major matrix to cuda memory

    auto b = generate_row_random_matrix(nx,1);
    auto cuda_b = allocate_cuda_mat<float>(nx,1);
    host_to_cuda(cuda_b,b[0],nx,1); //moves row major matrix to cuda memory
    auto cuda_b_res = allocate_cuda_mat<float>(nx,1);
    auto cuda_kernel_mat = allocate_cuda_mat<float>(nx,ny);
    auto cuda_kernel_mat_2 = allocate_cuda_mat<float>(nx,ny);

    auto cuda_b_res_2 = allocate_cuda_mat<float>(nx,1);


//    print_row_mat(b,nx,1); //figure out how to move pointer of pointer to device!

//    print_mat_cuda<float><<<grid_X,BLOCK_SIZE>>>(cuda_mat_x);
//    cudaDeviceSynchronize();
//    rbf_1d_kernel<<<grid_X,BLOCK_SIZE>>>(cuda_mat_x,cuda_mat_x,cuda_mat_res);
//    int grid_X = max((int)ceil((float)nx/(float)BLOCK_SIZE),1);
    dim3 blockSize;
    int denonminator = std::max(1,(int) (nd*sizeof(float)));
    blockSize.x = min(BLOCK_SIZE,min(MAXTHREADSPERBLOCK,(int) ( (float)SHAREDMEMPERBLOCK / float(denonminator))));
    dim3 gridSize;
    gridSize.x = nx / blockSize.x + (nx % blockSize.x == 0 ? 0 : 1);
    printf("%i \n",gridSize.x );

    rbf_1d_reduce_shared<<<gridSize, blockSize,blockSize.x * (nd) * sizeof(float)>>>(cuda_mat_x, cuda_mat_x, cuda_b, cuda_b_res);
    cudaDeviceSynchronize();
    auto cpu_res = cuda_to_host_and_pointer(cuda_b_res,nx,1);
    print_mat(std::cout,cpu_res,nx,1); //ok does something but very incorrectly
    printf("--------------------------------------------------------------------------------------------\n");
    rbf_1d_reduce_simple<<<gridSize, blockSize,blockSize.x * (nd) * sizeof(float)>>>(cuda_mat_x, cuda_mat_x, cuda_b, cuda_b_res_2);
    cudaDeviceSynchronize();
    auto cpu_res_2 = cuda_to_host_and_pointer(cuda_b_res_2,nx,1);
    print_mat(std::cout,cpu_res_2,nx,1); //ok does something but very incorrectly
**/

//    rbf_1d_kernel_shared<<<gridSize, blockSize,blockSize.x * (nd) * sizeof(float)>>>(cuda_mat_x, cuda_mat_x, cuda_kernel_mat);
//    cudaDeviceSynchronize();
//    auto cpu_res = cuda_to_host_and_pointer(cuda_kernel_mat,nx,ny);
//    print_mat(std::cout,cpu_res,nx,nx); //ok does something but very incorrectly
//    printf("\n");
//    rbf_1d_kernel<<<gridSize, blockSize,blockSize.x * (nd) * sizeof(float)>>>(cuda_mat_x, cuda_mat_x, cuda_kernel_mat_2);
//    cudaDeviceSynchronize();
//    auto cpu_res_2 = cuda_to_host_and_pointer(cuda_kernel_mat_2,nx,ny);
//    print_mat(std::cout,cpu_res_2,nx,nx); //ok does something but very incorrectly

//


//    auto cuda_kernel_mat = allocate_cuda_mat<float>(N,N);
//
//    std::cout<< grid_Y<<std::endl;
//    std::cout<< grid_X<<std::endl;
//
//    dim3 grid(grid_Y,grid_X);
//    dim3 block_dim(BLOCK_SIZE,BLOCK_SIZE);
//    host_to_cuda(cuda_mat,mat,N,M);
//    print_mat_cuda<float><<<grid,block_dim>>>(cuda_mat,M,N);
//    cudaDeviceSynchronize();


//    int n = 10;
//    int m = 10;
//    cumatrix<float> a(n, m), b(n, m),f(1,1); //this calls the constructors
//    cumatrix<float> c = a * b; //Do matmul, constructor, we initialize a new cumatrix<float> which is c in the function.
//
////    f = c; //this calls the copy ass
////    cumatrix<float> bub(a); //this calls the copy constructor
////    cumatrix<float> foo = test_move_constructor(cumatrix<float>(1,1)); //this calls the move constructor
////    c =  test_move_constructor(cumatrix<float>(1,1)); //this calls the move assignment op
////    torch::Tensor output = ffm_obj_test*b;
////    exact_MV<float> exact_obj_test = exact_MV<float>(X,X,ls,op,lambda,device_cuda);
////    torch::Tensor output_ref = exact_obj_test*b;
//    torch::Tensor loss,grad,b_inv;
////    std::tie(b_inv,tridiag_matrix) = CG(ffm_obj_test,b,(float) 1e-6,(int) 100,true);
////    std::tie(log_det,trace) = trace_and_log_det_calc(fmm_obj,ffm_obj_grad,(int)10,(int)50,(float)1e-6);
////    std::cout<<log_det<<std::endl;
////    std::cout<<trace<<std::endl;
//    std::tie(loss,grad,b_inv)=calculate_loss_and_grad<float>(fmm_obj,ffm_obj_grad,b,T,max_its,tol);
//
////    log_det = calculate_one_lanczos_triag(tridiag_matrix);
//    std::cout<<loss<<std::endl;
//    std::cout<<grad<<std::endl;
//    std::cout<<b_inv<<std::endl;


//    X_data=X,X,ls,op,lambda,device_cuda
//    torch::Tensor output = FFM_XY<float>(X,X,b,device_cuda,ls,op);
//    torch::Tensor output_ref = torch::zeros_like(output);
//    X = X.to(device_cuda);
//    b = b.to(device_cuda);
//    output_ref = output_ref.to(device_cuda);
//    rbf_call<float>(X, X, b, output_ref,ls,op, false);
//
//    std::cout<<output<<std::endl;
//    printf("--------------------------------------\n");
//    std::cout<<output_ref<<std::endl;

//template <typename scalar_t>
//void far_field_compute(torch::Tensor & far_field_interactions,
//                       n_tree_big & x_box,
//                       n_tree_big & y_box,
//                       torch::Tensor & output,
//                       torch::Tensor &b,
//                       const std::string & device_gpu,
//                       torch::Tensor &dist,
//                       scalar_t & ls,
//                       rbf_pointer<scalar_t> & op){
//    torch::Tensor chebnodes_1D = chebyshev_nodes_1D(laplace_nodes); //get chebyshev nodes, laplace_nodes is fixed now, should probably be a variable
//    torch::Tensor laplace_combinations = get_recursive_indices(laplace_nodes,nd); // get all possible combinations of chebyshev nodes. Total combintations should have dimension [laplace_nodes^nd,nd]
//    torch::Tensor cheb_data_X = (get_cheb_data<scalar_t>(chebnodes_1D,laplace_combinations)*x_box.edge/2+x_box.edge/2).to(device_gpu); //get the actual data given the indices
//    chebnodes_1D=chebnodes_1D.to(device_gpu);
//    laplace_combinations=laplace_combinations.to(device_gpu);
//    torch::Tensor unique_box_indices_Y,_inverse_indices_Y,_counts_Y,cheb_data_Y,unique_box_indices_X,_1,_2;
//    std::tie(unique_box_indices_Y,_inverse_indices_Y,_counts_Y) = torch::_unique2(far_field_interactions.slice(1, 1, 2), true, false);
//    std::tie(unique_box_indices_X,_1,_2) = torch::_unique2(far_field_interactions.slice(1, 0, 1), true, false);
//
//    /*
//     * for all unique indices do a transformation, i.e. L_y*u.
//     */
//    std::map<long, torch::Tensor> Y_box_transforms;
//    torch::Tensor laplace_low_rank,y_subset,b_subset;
//    auto unique_box_indices_Y_accessor = unique_box_indices_Y.accessor<long,1>();
//    auto unique_box_indices_X_accessor = unique_box_indices_X.accessor<long,1>();
//
//    for (int i=0; i<unique_box_indices_Y.numel();i++){//parallelize!
//        Y_box_transforms[unique_box_indices_Y_accessor[i]] = apply_laplace_interpolation<scalar_t>(
//                y_box,
//                b,
//                unique_box_indices_Y_accessor[i],
//                device_gpu,
//                chebnodes_1D,
//                laplace_combinations,
//                false);
//    }
//
//    std::cout<<Y_box_transforms[0]<<std::endl;
//    std::cout<<Y_box_transforms[1]<<std::endl;
//
//    /*
//     * Do all low rank interactions,i.e. T*(L_y*u) we apply T here.
//     */
//    std::map<long, torch::Tensor> results; //Store/accumulate results according to unique X-boxes
//    for (int i=0; i<unique_box_indices_X.numel();i++){
//        results[unique_box_indices_X_accessor[i]] = torch::zeros({cheb_data_X.size(0), b.size(1)}).toType(torch::kFloat32).to(device_gpu);
//    }
//    torch::Tensor xy; //distance offset
//    long m,n;
//    auto far_field_accessor = far_field_interactions.accessor<long,2>();
//    for (int i =0; i<dist.size(0);i++){ //parallelize
//        xy = dist.slice(0,i,i+1);
//        m = far_field_accessor[i][1];
//        n = far_field_accessor[i][0];
//        results[unique_box_indices_X_accessor[n]]+=low_rank_exact<scalar_t>(
//                cheb_data_X,
//                Y_box_transforms[unique_box_indices_Y_accessor[m]],
//                xy,
//                device_gpu,
//                ls,
//                op
//                );
//    }
//
//    std::cout<<results[0]<<std::endl;
//    std::cout<<results[1]<<std::endl;
//
//
//    /*
//     * Finally transform back to regular dimensionality i.e. Apply L_x *(T*(L_y*u))
//     */
//    std::vector<torch::Tensor> final_res = {}; // We cache all the results...
//    std::vector<torch::Tensor> final_indices = {}; // ... and remember the index we applied it to!
//
//    for (int i=0; i<unique_box_indices_X.numel();i++){//parallelize!
//        final_res.push_back(apply_laplace_interpolation<scalar_t>(
//                x_box,
//                results[unique_box_indices_X_accessor[i]],
//                unique_box_indices_X_accessor[i],
//                device_gpu,
//                chebnodes_1D,
//                laplace_combinations,
//                true).to("cpu"));
//        final_indices.push_back(x_box.n_roons[unique_box_indices_X_accessor[i]].row_indices);
//        }
//    torch::Tensor update = torch::cat({final_res});
//    torch::Tensor rows = torch::cat({final_indices});
//    update_2d_rows_cpu<scalar_t>(output,update,rows); //since we have not done accumulation ,we do accumulation here...
//};

//template <typename scalar_t>
//torch::Tensor low_rank_exact(torch::Tensor & cuda_X_job,
//                    torch::Tensor & cuda_b_job,
//                    torch::Tensor & distance,
//                     const std::string & device_gpu,
//                     scalar_t & ls,
//                     rbf_pointer<scalar_t> & op
//                     ){
//    distance = distance.to(device_gpu);
//    torch::Tensor output_job = torch::zeros_like(cuda_b_job).toType(torch::kFloat32);
//    torch::Tensor cuda_Y_job = cuda_X_job + distance;
//    rbf_call<scalar_t>(cuda_X_job, cuda_Y_job, cuda_b_job, output_job,ls,op);
//    return output_job;
//}
//template <typename scalar_t>
//torch::Tensor apply_laplace_interpolation(
//        n_tree_big& n_tree,
//        torch::Tensor &b,
//        long i ,
//        const std::string & device_gpu,
//        torch::Tensor & nodes,
//        torch::Tensor & laplace_indices,
//        const bool tranpose_mode=false){
//    torch::Tensor box_data = n_tree.data.index({n_tree.n_roons[i].row_indices});
//    box_data = ((2 / n_tree.edge) * (box_data - n_tree.n_roons[i].center)).to(device_gpu);
//    dim3 blockSize,gridSize;
//    int memory;
//    std::tie(blockSize,gridSize,memory) = get_kernel_launch_params<scalar_t>(nd, box_data.size(0));
//
//    if (not tranpose_mode){
//        torch::Tensor laplace_low_rank = torch::zeros({b.size(1),laplace_indices.size(0)}).to(device_gpu);
//        torch::Tensor b_data = b.index({n_tree.n_roons[i].row_indices}).to(device_gpu);
//        laplace_interpolation<scalar_t><<<gridSize,blockSize>>>(
//                box_data.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),
//                b_data.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),
//                nodes.packed_accessor32<scalar_t,1,torch::RestrictPtrTraits>(),
//                laplace_indices.packed_accessor32<int,2,torch::RestrictPtrTraits>(),
//                laplace_low_rank.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>());
//        cudaDeviceSynchronize();
//        return laplace_low_rank.t_();
//    }else{
//        torch::Tensor high_rank_res = torch::zeros({box_data.size(0),b.size(1)}).to(device_gpu);
//        torch::Tensor& b_data =b;
//        laplace_interpolation_transpose<scalar_t><<<gridSize,blockSize>>>(
//                box_data.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),
//                b_data.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),
//                nodes.packed_accessor32<scalar_t,1,torch::RestrictPtrTraits>(),
//                laplace_indices.packed_accessor32<int,2,torch::RestrictPtrTraits>(),
//                high_rank_res.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>());
//        cudaDeviceSynchronize();
//        return high_rank_res;
//    }
//}

//
//template <typename scalar_t>
//void near_field_compute(torch::Tensor & near_field_interactions,
//                        n_tree_big & x_box,
//                        n_tree_big & y_box,
//                        torch::Tensor & output,
//                        torch::Tensor &b,
//                        const std::string & device_gpu,
//                        scalar_t & ls,
//                        rbf_pointer<scalar_t> & op){
//    torch::Tensor unique_box_indices_Y,_inverse_indices_Y,_counts_Y;
//    std::tie(unique_box_indices_Y,_inverse_indices_Y,_counts_Y) = torch::_unique2(near_field_interactions.slice(1,1,2),true,true);
//    std::vector<torch::Tensor> job_vector;
//    auto unique_box_indices_Y_accessor = unique_box_indices_Y.accessor<long,1>();
//    for (int i=0; i<unique_box_indices_Y.numel();i++){
//        job_vector.push_back(near_field_interactions.slice(1,0,1).index({_inverse_indices_Y==unique_box_indices_Y_accessor[i]}));
//    }
//    replace_box_index_with_data_index_X(job_vector,x_box);
////    cudaStream_t streams[MAX_STREAMS];
//    torch::Tensor & X_data = x_box.data;
//    torch::Tensor & Y_data = y_box.data;
//
//    torch::Tensor Y_inds_job,cuda_Y_job,cuda_b_job,cuda_X_job,X_inds_job,output_job;
//    std::vector<torch::Tensor> results = {};
//
//    for (int i=0; i<unique_box_indices_Y.numel();i++){ //for each Y-box, find all the "common" X-boxes, stack the X-boxes and compute exactly.
//        Y_inds_job = y_box.n_roons[unique_box_indices_Y_accessor[i]].row_indices;
//        cuda_Y_job = Y_data.index({Y_inds_job}).to(device_gpu); //breaks on seccond iteration...
//        cuda_b_job = b.index({Y_inds_job}).to(device_gpu);
//        X_inds_job = job_vector[unique_box_indices_Y_accessor[i]];
//        cuda_X_job = X_data.index(X_inds_job).to(device_gpu);
//        output_job = torch::zeros({X_inds_job.size(0),output.size(1)}).to(device_gpu);
//        rbf_call<scalar_t>(cuda_X_job, cuda_Y_job, cuda_b_job, output_job,ls,op);
//        results.push_back(output_job.to("cpu"));
//    }
//    torch::Tensor update = torch::cat({results});
//    torch::Tensor rows = torch::cat({job_vector});
//    update_2d_rows_cpu<scalar_t>(output,update,rows);
//};
//Make smarter implementation for tmrw using pointers to X and b rather than accessing the entire thing
//torch::Tensor concat_X_box_indices(n_tree_big &x_box, torch::Tensor & box_indices){
//    std::vector<torch::Tensor> cat = {};
//    for (int i=0; i<box_indices.numel();i++){
//        cat.push_back(x_box.n_roons[box_indices.accessor<long,1>()[i]].row_indices);
//    }
//    return torch::cat(cat,0);
//}
//
//void replace_box_index_with_data_index_X(std::vector<torch::Tensor> &job_vector, n_tree_big & x_box) {
//    for (auto &el : job_vector) {
//        el = concat_X_box_indices(x_box, el);
//    }
//}

//    }